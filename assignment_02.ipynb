{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1cfb22-4b03-4682-9a21-bca5cb05c682",
   "metadata": {},
   "source": [
    "# Assignment 2: sentiment analysis of SUTD Reddit\n",
    "**Assignment due 21 March 11:59pm**\n",
    "\n",
    "Welcome to the second assignment for 50.055 Machine Learning Operations. These assignments give you a chance to practice the methods and tools you have learned. \n",
    "\n",
    "**This assignment is an individual assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**\n",
    "- The completed notebook, including your added code and generated output and a labeled dataset which you create in the assignment will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster to solve and test the assignment.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. There is a maximum of 150 points for this assignment.\n",
    "\n",
    "**ChatGPT policy** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980439b6-d91e-467e-a0b8-45a0d6637c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing all required packages\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "! pip install transformers[torch]==4.37.2\n",
    "! pip install datasets==2.17.1\n",
    "! pip install seaborn==0.13.2\n",
    "! pip install pyarrow==15.0.0\n",
    "! pip install scikit-learn==1.4.0\n",
    "! pip install emoji==0.6.0\n",
    "! pip install accelerate==0.27.2\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272f7c4-8dec-4ec8-986c-2ee28ed366f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required packages\n",
    "# ----------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda5aa5-1297-4767-b36e-05fe6df2cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f84068-a112-46a0-b531-494a903eccae",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "Sentiment analysis is a natural language processing technique that identifies the polarity of a given text. There are different flavors of sentiment analysis, but one of the most widely used techniques labels data into positive, negative and neutral. We have already encountererd sentiment analysis in the hands-on sessions.\n",
    "\n",
    "In this assignment, you will conduct sentiment analysis on posts and comments from the SUTD subreddit. You will run experiments with pre-trained sentiment models, evaluate their performance and simulate improving the model by re-training it with newly annotated data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b9f1f-5862-40c0-8b10-a1b7c23fac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SUTD subreddit data set as dataframe\n",
    "# posts and comments have been downloaded from https://www.reddit.com/r/sutd/\n",
    "\n",
    "df_submissions = pd.read_parquet('reddit_submissions.parquet.gzip').set_index(\"Id\")\n",
    "df_comments = pd.read_parquet('reddit_comments.parquet.gzip').set_index(\"CommentId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcaa3cc-c7f7-409a-9303-75bfb97d106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the data. The data schema is as follows.\n",
    "\n",
    "# Submissions\n",
    "# Id - unique id for submission\n",
    "# Title - text of the submission title\n",
    "# Upvotes - upvotes on this submission\n",
    "# Created - date time of submission creation date and time\n",
    "\n",
    "# Comments\n",
    "# CommentId - unique id for comment\n",
    "# Comment - text content of the comment\n",
    "# CommentCreated - date time of comment creation date and time\n",
    "# Id - unique id for submission on which the comment was posted\n",
    "\n",
    "# See the Reddit API documentation for details https://www.reddit.com/dev/api/\n",
    "df_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cef690-ef3c-437e-96e5-28ed0554f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3ab42-155a-4efb-ab9a-9d65f2b6e4fb",
   "metadata": {},
   "source": [
    "You can read the SUTD reddit submissions in your web browser by navigating to \n",
    "https://www.reddit.com/r/sutd/comments/{Id}\n",
    "\n",
    "\n",
    "### QUESTION: \n",
    "How easy is it to make sense of the submissions and comments? Is it easier to understand the posts when you read them in the browser? \n",
    "Explain why or why not (max 100 words)\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (5 points)---**\n",
    "\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bf3cf-4dae-4d27-85f7-f3172464838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Join the data frames into a joined data_frame 'df_reddit' which  contains both submissions and comments. \n",
    "# Each row should contain a submission paired with one associated comment. Comments that do not have a matching submission shall be dropped. The joined data frame should have the following schema.\n",
    "\n",
    "# Submissions\n",
    "# Id - unique id for submission\n",
    "# Title - text of the submission title\n",
    "# Upvotes - upvotes on this submission\n",
    "# Created - date time of submission creation date and time\n",
    "# CommentId - unique id for comment, comment is posted for this submission\n",
    "# Comment - text content of the comment\n",
    "# CommentCreated - date time of comment creation date and time\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54938b4a-6b0d-4f59-a652-f037eb222564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 rows of the joined data frame\n",
    "df_reddit.head(10)\n",
    "\n",
    "# Hint: submission will be duplicated as many times as there are comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135b330-daeb-4557-8de8-45fa20bc26fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run a pre-trained sentiment analysis model on the submissions and comments\n",
    "# A convenient way to execute pre-trained models for standard tasks are Huggingface pipelines\n",
    "# Here we run a standard sentiment analysis pipeline on the first ten submission titles \n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=0)\n",
    "print(sentiment_pipeline(list(df_submissions['Title'][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b7c9c-fce9-4a5d-ae9d-0ba3ebb92685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Complete the function 'analyse_sentiment' which takes a data frame, a Huggingface sentiment pipeline object \n",
    "# and a target column name and adds two columns 'Label' and 'Score' to the data frame in place.\n",
    "# pass the provided tokenizer arguments to the pipeline\n",
    "# The new columns should contain the sentiment labels and scores, respectively.\n",
    "\n",
    "\n",
    "def analyse_sentiment(df, sentiment_pipeline, column):\n",
    "    tokenizer_kwargs = {'padding':True, 'truncation':True, 'max_length':128,}\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8915fe-1ed9-4b4a-82ed-fa34de18427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment labels and scores to the submissions and comments dataframes\n",
    "analyse_sentiment(df_submissions, sentiment_pipeline, 'Title')\n",
    "analyse_sentiment(df_comments, sentiment_pipeline, 'Comment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b1a32-9c90-4f05-aa29-4bdd22ad5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe \n",
    "df_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd22ae4-a562-4be6-bf48-8c13272f0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe \n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840e3c2-6c18-45c0-b9b7-16291dcf5bf4",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "From a first inspection of the results, what problems can you see with our current sentiment analysis?\n",
    "What model is used for the sentiment analysis and how was is trained?\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (5 points) ---**\n",
    "\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73f29c-9728-4a19-8d5a-9e7440d56f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Update the sentiment pipeline to use the model \"finiteautomata/bertweet-base-sentiment-analysis\" from Huggingface\n",
    "# The model should output three classes: 'POS', 'NEG', 'NEU'\n",
    "# Store the model name in separate variable \"model_name\"\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points) ---\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f89112",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "\n",
    "Explain why this model is better suited for the task (max 100 words).\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (5 points) ---**\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e71586-a9b8-4e62-ad0a-df55c2999c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run the sentiment analysis of submissions and comments\n",
    "analyse_sentiment(df_submissions, sentiment_pipeline, 'Title')\n",
    "analyse_sentiment(df_comments, sentiment_pipeline, 'Comment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d66588-1b79-4d05-a00d-3c09f5012bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe \n",
    "df_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37daf3-c654-47f0-8390-7787ef41ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframe \n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665eb864-d07e-465d-8fbd-566c5653f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: What is the time frame covered by the data set, i.e. what is the earliest time of a submission or comment and what is the most recent time?\n",
    "# Find the earliest and latest timestamp and print them\n",
    "#--- ADD YOUR SOLUTION HERE (8 points)---\n",
    "\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216faae-504f-4fb3-bb0c-6af37db26a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: How did the volume of posts on the SUTD subreddit change over the years?\n",
    "# Create a bar chart diagram that plots the number of submissions per year on the y-axis and the year on the x-axis.\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (8 points) ---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12eebc2-6953-4d7a-952a-d185d4dccf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: What is the distribution of positive, neutral and negative sentiment?\n",
    "# Create a bar chart diagram that plots the number of submissions on the y-axis and the sentiment label on the x-axis.\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2cd28-c567-4d97-abca-d62f43f633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: What is the distribution of positive, neutral and negative sentiment for comments?\n",
    "# Create a bar chart diagram that plots the number of comments on the y-axis and the sentiment label on the x-axis.\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f96c87-cfec-4d0e-96da-102a30db0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: combine submission titles and comments for the time period from 2021 until today into one data frame.\n",
    "# The resulting data frame 'df_text' should have the following schema\n",
    "\n",
    "# Id - unique id of the comment or the submissions, this column is the index of the data frame \n",
    "# Text - text content of the comment or the submission title\n",
    "# Created - date time when submission or comment was created\n",
    "# Label - sentiment label as predicted by ML\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7ec71-6c9d-40e9-bc24-cbe0acba0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the resulting data frame\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ef9b0-dd98-4d16-8676-e280122f62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: sort the data frame by date time descending and save it in the same variable\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (3 points)---\n",
    "\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cba6c-5d42-4606-8a35-ef4a7253ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the resulting data frame\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c1ef1-559f-4db8-8867-c8aca481d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame to csv\n",
    "df_text.to_csv(\"reddit.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27bb73-f483-4364-9b9a-59493031cfcf",
   "metadata": {},
   "source": [
    "Download the csv file and open it in a spreadsheet application or text editor. \n",
    "\n",
    "Inspec the first 10-20 entries in the list to get a feeling for the data domain.\n",
    "\n",
    "### QUESTION: \n",
    "Write a short labeling guide for annotating the SUTD reddit data with sentiment labels. \n",
    "You can write the labeling guide in a bullet point format and should have 5-10 points.\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points)---**\n",
    "\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24d7a-ecbd-4da1-b642-7c28db4e06eb",
   "metadata": {},
   "source": [
    "## Label the data\n",
    "Add a new column 'HumanLabel' to the csv file and label the 500 most recent entries, including the first 10-20 you inspected to create the label guide, using a spreadsheet application (Excel, Google Docs, Numbers) or just a text editor. \n",
    "\n",
    "### QUESTION: \n",
    "What were some of the ambiguous cases or corner cases you encountered?\n",
    "List 3-5 issues\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (30 points)---**\n",
    "\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "\n",
    "Upload your 500 labeled instances as **reddit_labeled.csv** to JupyterLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637838e-9741-4c6f-a889-e76dfbe25ead",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "Compare your human-corrected labels with the original predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2a4a7-78ea-4942-afe3-cf20d177eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# QUESTION: Read the 500 labeled rows from the CSV file into a dataframe \"df_labeled\". \n",
    "# The data frame should have this schema.\n",
    "\n",
    "# Id - unique id of the comment or the submissions, Id is the index of the data frame \n",
    "# Text - text content of the comment or the submission title\n",
    "# Created - date time when submission or comment was created\n",
    "# Label - sentiment label as predicted by ML\n",
    "# HumanLabel - manually reviewed 'gold sentiment label'\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbffcdc3-fa7d-442f-9250-0950e853d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data was loaded correctly\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480121a-ae2a-462b-9890-4b6f029a5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the labeled data into two chunks, ordered by time\n",
    "df_labeled.sort_values('Created', ascending=True, inplace=True)\n",
    "\n",
    "df_labeled1 = df_labeled[:250]\n",
    "df_labeled2 = df_labeled[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805392d-3dc0-46d3-83da-83070dba1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the each split is 250 instances and that they don't overlap\n",
    "df_labeled1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429f03f-0027-4469-ae2f-15c69504ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbc118-5449-4810-be11-969826c6760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the agreement between the predicted labels and your manually created \"gold labels\" in split 1. \n",
    "# Compute scores for overall accuracy as well as precision/recall/f1 score for each label class\n",
    "# Print all scores \n",
    "\n",
    "print(sklearn.metrics.classification_report(df_labeled1[\"Label\"], df_labeled1[\"HumanLabel\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dd320-6228-467b-9850-3d1c2201eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the agreement between the predicted labels and your manually created \"gold labels\" in split 2. \n",
    "# Compute scores for overall accuracy as well as precision/recall/f1 score for each label class\n",
    "# Print all scores \n",
    "\n",
    "print(sklearn.metrics.classification_report(df_labeled2[\"Label\"], df_labeled2[\"HumanLabel\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7958c9e-6aab-4066-a0f0-0296e08c0935",
   "metadata": {},
   "source": [
    "## Retrain sentiment model\n",
    "\n",
    "Now let us use the data in df_labeled1 to try improve the sentiment classifier.\n",
    "Train the Huggingface model you have chosen with the 250 examples and your human gold labels.\n",
    "\n",
    "Start by converting the data from data frames into a 2 Huggingface datasets. \n",
    "- dataset1 : a Huggingface dataset object which includes the data from dataframe df_labeled1\n",
    "- dataset2 : a Huggingface dataset object which includes the data from dataframe df_labeled2\n",
    "\n",
    "\n",
    "In each dataset, there should be the following fields\n",
    "- text : the text of the reddit submission or comment\n",
    "- label: the human gold label, encoded as integer\n",
    "\n",
    "With these dataset we will simulate the process of improving a model in production. Dataset1 is simulating a batch of data which we observed in production, annotated and then use to improve the model. We evaluate the change on the new training data and on the next batch of production data, simulated by dataset2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb36c6b-5f51-406a-848a-1f81e54c3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(df, pipeline):\n",
    "    # drop predicted label column\n",
    "    df = df.drop(\"Label\", axis=1)\n",
    "    # convert string labels to integers as column 'label' using the sentiment pipeline config\n",
    "    label_id_mapping = lambda label: pipeline.model.config.label2id[label]\n",
    "    df['label'] = df['HumanLabel'].apply(label_id_mapping)\n",
    "    return df\n",
    "\n",
    "df_labeled1 = convert_label(df_labeled1, sentiment_pipeline)\n",
    "df_labeled2 = convert_label(df_labeled2, sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fcb8a-17fd-4a9e-99ae-c1ae7cdb8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Convert the text and human labels from the data frame to a huggingface dataset format\n",
    "# create a huggingface 'dataset1' from data frame 'df_labeled1' and 'dataset2' from data frame 'df_labeled2' \n",
    "#\n",
    "# each dataset has the following fields\n",
    "# text : the text of the reddit submission or comment\n",
    "# label: the human gold label, encoded as integer\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415c538-f5ea-44be-9a23-47e2e4867ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first example\n",
    "dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106f683-1c93-414f-b7ba-31a3722367da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and tokenize data set\n",
    "# \n",
    "# QUESTION: Load the required tokenizer from Huggingface into a variable 'tokenizer'\n",
    "# Then tokenize 'dataset1' into 'tokenized_dataset1' and 'dataset2' into 'tokenized_dataset2'\n",
    "# Use the Huggingface libraries. Remember that we stored the model name in a variable \"model_name\"\n",
    "\n",
    "# helper function for tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (5 points)---\n",
    "\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09087fb6-1cc6-457c-b601-316d7bb3b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Hugging model for classification initialized with the sentiment model you have chosen\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (3 points)---\n",
    "\n",
    "#------------------------------\n",
    "# Hint: make sure your model corresponds to your tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d865dca-a413-4060-ac4e-a49039262f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom metrics that computes precision, recall, f1, accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "   # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbf727-1479-4b81-988a-c18eba4de126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# QUESTION: configure the training parameters using the Hugginface TrainingArguments class\n",
    "# - set the output directory to \"finetuning-reddit\"\n",
    "# - do not report training metrics to an external experiment tracking service\n",
    "# - learning rate to 2e-5, \n",
    "# - set weight decay to 0.01\n",
    "# - set logging_steps to 10,\n",
    "# - set evaluation_strategy to \"steps\",\n",
    "# - set epochs to 3\n",
    "\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (3 points)---\n",
    "\n",
    "#------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef1bb5-565d-4d9d-ad4a-cfba3f6d47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trainer\n",
    "# train on the split dataset1\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset1,\n",
    "    eval_dataset=tokenized_dataset2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f928280-ea16-4c06-85a2-ecb29d6aad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dataset1 set before training \n",
    "predictions = trainer.predict(tokenized_dataset1)\n",
    "print(sklearn.metrics.classification_report(predictions.predictions.argmax(-1), dataset1['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c369b-9c04-4e52-a9f2-511af6d73c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dataset2 set before training \n",
    "predictions = trainer.predict(tokenized_dataset2)\n",
    "print(sklearn.metrics.classification_report(predictions.predictions.argmax(-1), dataset2['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d891ae-4a62-44d1-8cf1-d05a1bc75074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4559b-bbbc-4ea5-981a-1b833e1c35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dataset1, i.e the training set again\n",
    "preditions = trainer.predict(tokenized_dataset1)\n",
    "print(sklearn.metrics.classification_report(preditions.predictions.argmax(-1), dataset1['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471dbc36-4419-4f56-8406-ef4b2218d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dataset2 set i.e. the test set again\n",
    "predictions = trainer.predict(tokenized_dataset2)\n",
    "print(sklearn.metrics.classification_report(predictions.predictions.argmax(-1), dataset2['label']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d39da-72df-410e-adb4-325c6be590d4",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "Has the model improved performance on the first batch of data? Does the model generalize well to the next batch of data?\n",
    "Do you see any signs of overfitting or underfitting based on the evaluation scores\n",
    "Explain why or why not\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (5 points)---**\n",
    "\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e7d14-f997-4022-ab46-2cb1ac247073",
   "metadata": {},
   "source": [
    "### QUESTION: \n",
    "Is the model good enough to be used for practical applications?\n",
    "Given the results you have so far, what additional measures would you recommend to continuously improve the SUTD reddit sentiment classifier? What other functionalities beyond sentiment could be useful? Write a paragraph (max 200 words) to explain your choice\n",
    "\n",
    "**--- ADD YOUR SOLUTION HERE (10 points)---**\n",
    "\n",
    "\n",
    "------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc57acf-c65a-4ba7-9d9c-9b212251e542",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "This concludes assignment 2.\n",
    "\n",
    "Please submit this notebook with your answers and the generated output cells as a **Jupyter notebook file** and the **text file reddit_labeled_STUDENT_NAME.csv** via github.\n",
    "\n",
    "\n",
    "1. Create a private github repository **sutd_5055mlop** under your github user.\n",
    "2. Add your instructors as collaborator: ddahlmeier and lucainiaoge\n",
    "3. Save your submission as assignment_02_STUDENT_NAME.ipynb and reddit_labeled_STUDENT_NAME.csv where STUDENT_NAME is your name in your SUTD email address.  \n",
    "4. Push the submission files to your repo \n",
    "5. Submit the link to the repo via eDimensions\n",
    "\n",
    "Example:<br/>\n",
    "Email: michael_tan@mymail.sutd.edu.sg<br/>\n",
    "STUDENT_NAME: michael_tan<br/>\n",
    "Submission file name: assignment_02_michael_tan.ipynb\n",
    "\n",
    "\n",
    "\n",
    "**Assignment due 21 March 2025 11:59pm**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20233b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
