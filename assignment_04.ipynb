{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99a8a55",
   "metadata": {},
   "source": [
    "# Group Project / Assignment 4: Instruction finetuning a Llama-3.2 model\n",
    "**Assignment due 21 April 11:59pm**\n",
    "\n",
    "Welcome to the fourth and final assignment for 50.055 Machine Learning Operations. The third and fourth assignment together form the course group project. You will continue the work on a chatbot which can answer questions about SUTD to prospective students.\n",
    "\n",
    "\n",
    "**This assignment is a group assignment.**\n",
    "\n",
    "- Read the instructions in this notebook carefully\n",
    "- Add your solution code and answers in the appropriate places. The questions are marked as **QUESTION:**, the places where you need to add your code and text answers are marked as **ADD YOUR SOLUTION HERE**. The assignment is more open-ended than previous assignments, i.e. you have more freedom how to solve the problem and how to structure your code.\n",
    "- The completed notebook, including your added code and generated output will be your submission for the assignment.\n",
    "- The notebook should execute without errors from start to finish when you select \"Restart Kernel and Run All Cells..\". Please test this before submission.\n",
    "- Use the SUTD Education Cluster to solve and test the assignment. If you work on another environment, minimally test your work on the SUTD Education Cluster.\n",
    "\n",
    "**Rubric for assessment** \n",
    "\n",
    "Your submission will be graded using the following criteria. \n",
    "1. Code executes: your code should execute without errors. The SUTD Education cluster should be used to ensure the same execution environment.\n",
    "2. Correctness: the code should produce the correct result or the text answer should state the factual correct answer.\n",
    "3. Style: your code should be written in a way that is clean and efficient. Your text answers should be relevant, concise and easy to understand.\n",
    "4. Partial marks will be awarded for partially correct solutions.\n",
    "5. Creativity and innovation: in this assignment you have more freedom to design your solution, compared to the first assignments. You can show of your creativity and innovative mindset. \n",
    "6. There is a maximum of 310 points for this assignment.\n",
    "\n",
    "**ChatGPT policy** \n",
    "\n",
    "If you use AI tools, such as ChatGPT, to solve the assignment questions, you need to be transparent about its use and mark AI-generated content as such. In particular, you should include the following in addition to your final answer:\n",
    "- A copy or screenshot of the prompt you used\n",
    "- The name of the AI model\n",
    "- The AI generated output\n",
    "- An explanation why the answer is correct or what you had to change to arrive at the correct answer\n",
    "\n",
    "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5e449",
   "metadata": {},
   "source": [
    "### Finetuning LLMs\n",
    "\n",
    "The goal of the assignment is to build a more advanced chatbot that can talk to prospective students and answer questions about SUTD.\n",
    "\n",
    "We will finetune a smaller 1B LLM on question-answer pairs which we synthetically generate. Then we will compare the finetuned and non-finetuned LLMs with and without RAG to see if we were able to improve the SUTD chatbot answer quality. \n",
    "\n",
    "We'll be leveraging `langchain`, `llama 3.2` and `Google AI STudio with Gemini 2.0`.\n",
    "\n",
    "Check out the docs:\n",
    "- [LangChain](https://docs.langchain.com/docs/)\n",
    "- [Llama 3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/)\n",
    "- [Google AI Studio](https://aistudio.google.com/)\n",
    "\n",
    "Note: Google AI Studio provides a lot of free tokens but has certain rate limits. Write your code in a way that it can handle these limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62530070",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "Use pip to install all required dependencies of this assignment in the cell below. Make sure to test this on the SUTD cluster as different environments have different software pre-installed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec75011-c8ef-4c31-9766-2e3e9834684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Install and import all required packages\n",
    "# The rest of your code should execute without any import or dependency errors.\n",
    "\n",
    "# **--- ADD YOUR SOLUTION HERE (10 points) ---**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563304ec",
   "metadata": {},
   "source": [
    "# Generate training data\n",
    "The first step of the assignment is generating synthetic question-answer pairs which can be used for finetuning an LLM model. \n",
    "Use the Google AI studio with the Gemini models to create -high-quality QA training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe166d-5ef6-4da9-995e-54afffc683c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Use langchain and the Google AI Studio APIs and a model from the Gemini 2.0 family\n",
    "# to create a text-generation chain that can produce and parse JSON output.\n",
    "# Test it by having the LLM generate a JSON array of 3 fruits\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edaed2",
   "metadata": {},
   "source": [
    "## Generate topics\n",
    "When generating data, it is often helpful to guide the generation process through some hierachical structure. \n",
    "Before we create question-answer pairs, let's generate some topics which the questions should be about.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee57ba-d3e2-45bd-9bc6-c7eeb354b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Create a function 'generate_topics' which generates topics which prospective students might care about.\n",
    "#\n",
    "# Generate a list of 20 topics \n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7787247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test topic generation\n",
    "print(generate_topics(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7efb98-7b40-4230-9eda-f344b4d4e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of 20 topics \n",
    "# We save a copy to disk and reload it from there if the file exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c4b4e",
   "metadata": {},
   "source": [
    "## Generate questions\n",
    "Now generate a set of questions about each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85706c9c-593d-459c-b86e-4011500cff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Create a function 'generate_questions' which generates quetions about a given topic. \n",
    "# Generate a list of 10 questions per topics. In total you should have 200 questions. \n",
    "#\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c9080-c3d0-458e-8c25-fa869fe0ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "print(generate_questions(\"Academic Reputation and Program Quality\", 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879af438-7482-4b66-ba7a-a888e554df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QUESTION: Now let's put it together and generate 10 questions for each topic. Save the questions in a local file.\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (20 points)---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dd1d4",
   "metadata": {},
   "source": [
    "## Generate Answers\n",
    "\n",
    "Now create answers for the questions. \n",
    "\n",
    "You can use the Google AI Studio Gemini model (assuming that they are good enough to generate good answers), your RAG system from assignment 3 or any other method you choose to generate answers for your question dataset.\n",
    "\n",
    "Note: it is normal that some LLM calls fail, even with retry, so maybe you end up with less than 200 QA pairs but it should be at least 160 QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e785dd-ab6f-41f8-961b-bf6f9ae24a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Generate answers to al your questions using Gemini, your SUTD RAG system or any other method.\n",
    "# Split your dataset in to 80% training and 20% test dataset.\n",
    "# Store all questions and answer pairs in a huggingface dataset `sutd_qa_dataset` and push it to your Huggingface hub. \n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26209b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the chain\n",
    "question = \"When was SUTD founded?\"\n",
    "\n",
    "# Now run the answer generation chain\n",
    "response = generate_answer(question)\n",
    "print(\"\\nModel Response:\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80228b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run the chain for all questions to collect context and generate answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dbba2-b593-48ca-abe0-ea8b2423bd19",
   "metadata": {},
   "source": [
    "# Finetune Llama 3.2 1B model\n",
    "\n",
    "Now use your SUTD QA dataset training data set to finetune a smaller Llama 3.2 1B LLM using parameter-efficient finetuning (PEFT). \n",
    "We recommend the unsloth library but you are free to choose other frameworks. You can decide the parameters for the finetuning. \n",
    "Push your finetuned model to Huggingface. \n",
    "\n",
    "Then we will compare the finetuned and non-finetuned LLMs with and without RAG to see if we were able to improve the SUTD chatbot answer quality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a684028-8dec-4297-a2c4-f659d2f32a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Finetune a Llama 3.2 1B model on the training split of your SUTD QA dataset.\n",
    "# You need to prepare your dataset accordingly and set the hyperparameters for the training.\n",
    "# Push your finetuned model to the Hugginface model hub {YOUR_HF_NAME}/llama-3.2-1B-sutdqa\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (50 points)---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268373a-9d77-4787-a0d7-a6df23cd94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Load a non-finetuned Llama 3.2 1B model and your finetuned SUTD QA Llama 3.2 1B model\n",
    "# Ask it a simple test question (e.g. \"What is special about SUTD?\") to check that both models can generated answers\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (10 points)---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out the llms\n",
    "\n",
    "query = \"What is special about SUTD?\"\n",
    "\n",
    "print(\"Question:\", query)\n",
    "response_base = llm_base.invoke(query,  pipeline_kwargs={\"max_new_tokens\": 512})\n",
    "print(\"Answer base:\", response_base)\n",
    "\n",
    "print(\"---------\")\n",
    "response_finetune = llm_finetune.invoke(query, pipeline_kwargs={\"max_new_tokens\": 512})\n",
    "print(\"Answer finetune:\", response_finetune)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc277373",
   "metadata": {},
   "source": [
    "# Integrate and evaluate\n",
    "\n",
    "Now integrate both the non-finetuned Llama 3.2 1B model and your finetuned model into your SUTD chatbot RAG system. \n",
    "Generate responses to the 20 questions you have collected in assignment 3 using these 4 appraoches\n",
    "1. non-finetuned Llama 3.2 1B model without RAG\n",
    "2. finetuned Llama 3.2 1B SUTD QA model without RAG\n",
    "3. non-finetuned Llama 3.2 1B model with RAG\n",
    "4. finetuned Llama 3.2 1B SUTD QA model with RAG\n",
    "\n",
    "Compare the responses and decide what system produces the most accurate and high quality responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da455368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Re-create the RAG chatbot system you have created in assignment 3 but with the Llama 3.2 1B (non-tuned and finetuned) models\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e2fdb-348c-45d4-8f6e-0bbf5136ec87",
   "metadata": {},
   "source": [
    "# Bonus points: LLM-as-judge evaluation \n",
    "\n",
    "Implement an LLM-as-judge pipeline to assess the quality of the different system (finetuned vs. non-fintuned, RAG vs no RAG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Implement an LLM-as-judge pipeline to assess the quality of the different system (finetuned vs. non-fintuned, RAG vs no RAG)\n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4cca8",
   "metadata": {},
   "source": [
    "# Bonus points: chatbot UI\n",
    "\n",
    "Implement a web UI frontend for your chatbot that you can demo in class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: Implement a web UI frontend for your chatbot that you can demo in class. \n",
    "\n",
    "#--- ADD YOUR SOLUTION HERE (40 points)---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576e6cc",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "This concludes assignment 4.\n",
    "\n",
    "Please submit this notebook with your answers and the generated output cells as a **Jupyter notebook file** via github.\n",
    "\n",
    "\n",
    "Every group member should do the following submission steps:\n",
    "1. Create a private github repository **sutd_5055mlop** under your github user.\n",
    "2. Add your instructors as collaborator: ddahlmeier and lucainiaoge\n",
    "3. Save your submission as assignment_04_GROUP_NAME.ipynb where GROUP_NAME is the name of the group you have registered. \n",
    "4. Push the submission files to your repo \n",
    "5. Submit the link to the repo via eDimensions\n",
    "\n",
    "\n",
    "\n",
    "**Assignment due 21 April 2025 11:59pm**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
